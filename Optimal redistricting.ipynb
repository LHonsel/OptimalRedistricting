{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from IPython.display import Image\n",
    "random.seed(9001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data for North Carolina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/luishonsel/Dropbox/DPI 610/Luis_Code')\n",
    "\n",
    "#Shapefile for North Carolina\n",
    "shapefile = gpd.read_file(\"../NC_VTD/NC_VTD.shp\").set_index('VTD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this project, one my my classmates, Luca Mingardi, prepared an adjacency matrix.  \n",
    "Such a matrix will be squared of size n, n being the number of VTDs (Voting Districts), and the (i,j) coordinate will be equal to 1 if i-th VTD is neighbour to j-th VTD.  \n",
    "In this code I will import the Adjacency matrix already done and will not code it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing adjacency matrix\n",
    "adj_matrix=pd.read_csv('DUMMY_adj_mat.csv').set_index('VTD_Key')\n",
    "\n",
    "\n",
    "list_VTD=adj_matrix.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing some basic statitics that we will use later on for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total state population\n",
    "state_pop=shapefile.PL10AA_TOT.sum()\n",
    "\n",
    "#maximum ratio of population per district our algorithm will authorize\n",
    "max_ratio=state_pop/12\n",
    "nb_district=13\n",
    "\n",
    "#population share per state if perfectly distributed\n",
    "state_avg_pop=state_pop/nb_district\n",
    "\n",
    "#democratic voters share per state if perfectly distributed\n",
    "state_avg_dem=shapefile.EL16G_GV_D.sum()/shapefile.PL10VA_TOT.sum()\n",
    "\n",
    "#african american voters share per state if perfectly distributed\n",
    "state_avg_black_share=shapefile.BPOP.sum()/state_pop\n",
    "\n",
    "shapefile['District']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the class object that will represent Districs , VTDs and remaining space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class District:\n",
    "    \n",
    "    def __init__(self,VTDs):\n",
    "        self.composition = VTDs\n",
    "        self.number_of_vtd = len(VTDs)\n",
    "        self.pop=shapefile.loc[VTDs].PL10AA_TOT.sum()\n",
    "        self.voting_pop=shapefile.loc[VTDs].PL10VA_TOT.sum()\n",
    "        self.pop_share=self.pop/state_pop\n",
    "        self.dem_pop=shapefile.loc[self.composition].EL16G_GV_D.sum()\n",
    "        self.dem_share=self.dem_pop/self.voting_pop\n",
    "        self.black_pop=shapefile.loc[VTDs].BPOP.sum()\n",
    "        self.black_share=shapefile.loc[VTDs].BPOP.sum()/self.pop\n",
    "        self.neighbors=remove(adj_matrix.index[adj_matrix.loc[VTDs].sum()>0],VTDs)\n",
    "        self.constraint=True\n",
    "        \n",
    "    def add(self,new_VTDs):\n",
    "        self.composition=self.composition+new_VTDs\n",
    "        self.pop+=shapefile.loc[new_VTDs].PL10AA_TOT.sum()\n",
    "        self.voting_pop+=shapefile.loc[new_VTDs].PL10VA_TOT.sum()\n",
    "        self.dem_pop=shapefile.loc[self.composition].EL16G_GV_D.sum()\n",
    "        self.dem_share=self.dem_pop/self.voting_pop\n",
    "        self.black_share=shapefile.loc[self.composition].BPOP.sum()/self.pop\n",
    "        self.neighbors=remove(adj_matrix.index[adj_matrix.loc[self.composition].sum()>0],self.composition)\n",
    "        self.constraint=self.pop<=max_ratio\n",
    "        self.pop_share=self.pop/state_pop\n",
    "        self.black_pop+=shapefile.loc[new_VTDs].BPOP.sum()\n",
    "        self.number_of_vtd += len(new_VTDs)\n",
    "        #rahouter des assert partout\n",
    "    def delete(self,del_VTDs):\n",
    "        self.composition=remove(self.composition,del_VTDs)\n",
    "        self.pop-=shapefile.loc[del_VTDs].PL10AA_TOT.sum()\n",
    "        self.voting_pop-=shapefile.loc[del_VTDs].PL10VA_TOT.sum()\n",
    "        self.dem_pop=shapefile.loc[self.composition].EL16G_GV_D.sum()\n",
    "        self.dem_share=self.dem_pop/self.voting_pop\n",
    "        self.black_share=shapefile.loc[self.composition].BPOP.sum()/self.pop\n",
    "        self.neighbors=remove(adj_matrix.index[adj_matrix.loc[self.composition].sum()>0],self.composition)\n",
    "        self.constraint=self.pop<=max_ratio\n",
    "        self.pop_share=self.pop/state_pop\n",
    "        self.number_of_vtd -= len(del_VTDs)\n",
    "        self.black_pop-=shapefile.loc[del_VTDs].BPOP.sum()\n",
    "        \n",
    "        \n",
    "class VTD:\n",
    "    \n",
    "    def __init__(self,vtd):\n",
    "        self.name = vtd\n",
    "        self.pop=shapefile.loc[vtd].PL10AA_TOT.sum()\n",
    "        self.voting_pop=shapefile.loc[vtd].PL10VA_TOT.sum()\n",
    "        self.dem_pop=shapefile.loc[vtd].EL16G_GV_D.sum()\n",
    "        self.dem_share=self.dem_pop/self.voting_pop\n",
    "        self.black_share=shapefile.loc[vtd].BPOP.sum()/self.pop\n",
    "        self.black_pop=shapefile.loc[vtd].BPOP.sum()\n",
    "        self.neighbors=adj_matrix.index[adj_matrix.loc[vtd]>0]\n",
    "        self.distric=np.nan\n",
    "    \n",
    "class Remaining_Space:\n",
    "    \n",
    "    def __init__(self,starting_points):\n",
    "        self.available_points=remove(list_VTD,starting_points)\n",
    "        \n",
    "    def remove(self,point):\n",
    "        self.available_points=remove(self.available_points,point)\n",
    "        \n",
    "    def add(self,point):\n",
    "        self.available_points+=point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create some useful function that will help us throughout the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(list1,list2):\n",
    "    return [i for i in list1 if i not in list2]\n",
    "\n",
    "def intersection(list1,list2):\n",
    "    return [i for i in list1 if i in list2]\n",
    "      \n",
    "def status(clusters):\n",
    "        return sum([i.constraint for i in clusters])\n",
    "    \n",
    "#evaluates the score for a certain distribution in one district        \n",
    "def cost_function_eval(vtd,district,weight_pop,weight_race,weight_vote):\n",
    "    \n",
    "    #difference in population repartition\n",
    "    tot_pop=district.pop+VTD(vtd).pop\n",
    "    pop_share=tot_pop/state_pop\n",
    "    avg_diff_pop=abs(pop_share-state_avg_pop)/state_avg_pop\n",
    "    \n",
    "    #difference in black population reparition\n",
    "    black_pop=district.black_pop+VTD(vtd).black_pop\n",
    "    black_pop_share=black_pop/tot_pop\n",
    "    avg_diff_black=abs(black_pop_share-state_avg_black_share)\n",
    "    \n",
    "    #difference in dem repartition\n",
    "    dem_pop=district.dem_pop+VTD(vtd).dem_pop\n",
    "    dem_pop_share=dem_pop/tot_pop\n",
    "    avg_diff_dem=abs(dem_pop_share-state_avg_dem)\n",
    "    \n",
    "    return(weight_pop*avg_diff_pop+weight_race*avg_diff_black+weight_vote*avg_diff_dem)\n",
    "    \n",
    "    \n",
    "#evaluates the score for a certain distribution in all districts        \n",
    "def cost_function_final(districts,weight_pop,weight_race,weight_vote):\n",
    "    cost=0\n",
    "    for dist in districts:\n",
    "        avg_diff_pop=abs(dist.pop-state_avg_pop)/state_avg_pop\n",
    "        avg_diff_black=abs(dist.black_share-state_avg_black_share)\n",
    "        avg_diff_dem=abs(dist.dem_share-state_avg_dem)\n",
    "        cost+=weight_pop*avg_diff_pop+weight_race*avg_diff_black+weight_vote*avg_diff_dem\n",
    "    return(cost)\n",
    "    \n",
    "    \n",
    "#indicates what would be the best VTD as a next step\n",
    "def best_vtd(district,points,weight_pop,weight_race,weight_vote):\n",
    "    cost=[]\n",
    "    for vtd in points:\n",
    "        cost.append(cost_function_eval(vtd,district,weight_pop,weight_race,weight_vote))\n",
    "    return(district.neighbors[np.argmin(cost)])    \n",
    "\n",
    "#useful function for printing\n",
    "def print_clusters(clusters):\n",
    "    for i in range(len(clusters)):\n",
    "        shapefile.loc[clusters[i].composition,'District']=i\n",
    "    return(shapefile.plot(column='District',figsize=(20,40)))  \n",
    "\n",
    "#find all parents of a VTD\n",
    "def get_parent_cluster(districts,vtd):\n",
    "    c=0\n",
    "    for i in range(len(districts)):\n",
    "        if vtd in districts[i].composition:\n",
    "            c=i\n",
    "    return c\n",
    "            \n",
    "#can our district lose this VTD to improve?\n",
    "def is_expendable(district,vtd):\n",
    "    neighbors=VTD(vtd).neighbors\n",
    "    neighbors_in_cluster=[i for i in neighbors if i in district.composition]\n",
    "    list1=VTD(neighbors_in_cluster[0]).neighbors\n",
    "    for vtd in neighbors_in_cluster:\n",
    "        list1=intersection(list1,VTD(vtd).neighbors)\n",
    "    a=0\n",
    "    for element in list1:\n",
    "        if element in district.composition:\n",
    "            a+=1\n",
    "    return a>=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method uses a classical heuristic algorithm widely used in optimization, the 2-OPT algorithm. Basically, this method starts from one feasible solution and explores all the neighboring ones that improve our cost function. Before going in detail into the 2-OPT algorithm, let’s first see how we generate feasible solutions to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: Random Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random start algorithm assign 13 random points to 13 distinct districts. At each iteration, each district gets added a new point, selected randomly among its neighboring available point. The algorithm stops when there are no available points left.\n",
    "0. Let S be the set of available point. S is comprised of all the states VTDs for now. We randomly select 13 points among S that will define the 13 distinct clusters.\n",
    "1. Remove those 13 points from S\n",
    "2. While S is not empty :  \n",
    "(a) For each cluster look at the neighboring points still in S  \n",
    "(b) Randomly select one of these points and add it to the cluster  \n",
    "(c) Remove this point from S\n",
    "\n",
    "The following figures give you the kind of redistricting you obtain though this method, through different iterations\n",
    "\n",
    "<img src=\"Picture1.png\">\n",
    "<img src=\"Picture2.png\">\n",
    "<img src=\"Picture3.png\">\n",
    "<img src=\"Picture4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to create the random start we need before the optimization    \n",
    "def random_init(length=1000,nb_of_districts=13,weight_pop=1,weight_race=1,weight_vote=1):\n",
    "    random_points=random.choices(list_VTD, k=nb_of_districts)\n",
    "    space=Remaining_Space(random_points)\n",
    "    clusters=[District([i]) for i in random_points]\n",
    "    state=status(clusters)\n",
    "    t=0\n",
    "    while state>0 and t<10000:\n",
    "        if len(space.available_points)!=0:\n",
    "            for clust in clusters:\n",
    "                try:\n",
    "                    possible_points=intersection(clust.neighbors,space.available_points)\n",
    "                    point=random.choices(possible_points, k=1)\n",
    "                    clust.add(point)\n",
    "                    space.remove(point)\n",
    "                except:\n",
    "                    pass\n",
    "                state=status(clusters)\n",
    "                \n",
    "        \n",
    "        t+=1\n",
    "    return(clusters,cost_function_final(clusters,weight_pop,weight_race,weight_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm: 2-OPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our feasible solutions to start, we can use the 2-OPT algorithm. The main idea is to explore all neighbouring solutions of our current solution. A neighbor will be a solution that only differs by one point. If the neighbor has a better score, it will be your new current solution. The algorithm runs until there are no more improvements possible.\n",
    "0. Let V be our current solution\n",
    "1. While there are changes in the following loop  \n",
    "(a) Look at every neighbor N of V  \n",
    "(b) If N has a better score than V, V :=N  \n",
    "(c) Otherwise move to another neighbouring solution\n",
    "\n",
    "<img src=\"example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use this method on top of the random start. it provides the following kind of solution:\n",
    "<img src=\"opt.png\">\n",
    "\n",
    "\n",
    "Evaluation\n",
    "This method presents different advantages, among which:  \n",
    "\n",
    "- Guaranteed contiguity: given the way the random start algorithm works, adding VTDs to clusters one at a time, and also given the way the 2-OPT algorithm works, we are assured that there will always be a path between any two points inside a cluster.  \n",
    "- The optimization part allows this method to provide better score than Weighted K-means or random start methods.\n",
    "- This model can provide different outputs depending on the metric we wan to prioritize.\n",
    "\n",
    "On the other hands, some of the drawbacks are:  \n",
    "- Locally optimal solution: Given that the 2-OPT method only examines the neigh- bours of a current solution, we can easily get stuck in a locally optimal solution and be far from a globally optimal one.  \n",
    "- Because of the way the districts are built, this methods does not perform as well as Weighted K means for the compactness score.  \n",
    "• This method is highly dependent on the starting point and we observe a high variance at each new output of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_1(districts,weight_pop=1,weight_race=1,weight_vote=1,tim=10):\n",
    "    curr_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "    for t in range(tim):\n",
    "        change=0\n",
    "        print('Completion status of main loop: ',t+1,' out of ',tim)\n",
    "        for i in range(len(districts)):\n",
    "            print('Completion status of sub loop: ',i+1,' out of ',len(districts))\n",
    "            to_visit=districts[i].neighbors  \n",
    "            for vtd in to_visit:\n",
    "                parent_cluster=get_parent_cluster(districts,vtd)\n",
    "                if is_expendable(districts[parent_cluster],vtd):\n",
    "                    old_cost=cost_function_final([districts[i],districts[parent_cluster]],weight_pop,weight_race,weight_vote)\n",
    "                    \n",
    "                    #cluster A\n",
    "                    cluster_A=District(districts[i].composition)\n",
    "                    cluster_A.add([vtd])\n",
    "                    \n",
    "                    #cluster B\n",
    "                    cluster_B=District(districts[parent_cluster].composition)\n",
    "                    cluster_B.delete([vtd])\n",
    "                    \n",
    "                    \n",
    "                    new_cost=cost_function_final([cluster_A,cluster_B],weight_pop,weight_race,weight_vote)\n",
    "                    if new_cost<old_cost:\n",
    "                        print('success')\n",
    "                        change+=1\n",
    "                        districts[i].add([vtd])\n",
    "                        districts[parent_cluster].delete([vtd])\n",
    "        if change==0:\n",
    "            break\n",
    "    new_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "    return(districts,print_clusters(districts),print('The score has been improved by',curr_cost-new_cost))\n",
    "    \n",
    "def optimization_2(districts,weight_pop=1,weight_race=1,weight_vote=1,tim=10):\n",
    "    curr_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "    for t in range(tim):\n",
    "        print('Completion status: ',t/tim)\n",
    "        change=0\n",
    "        for i in range(len(districts)):\n",
    "            to_visit=districts[i].neighbors  \n",
    "            for vtd in to_visit:\n",
    "                parent_cluster=get_parent_cluster(districts,vtd)\n",
    "                if is_expendable(districts[parent_cluster],vtd):\n",
    "                    parent_cluster=get_parent_cluster(districts,vtd)\n",
    "                    old_cost=cost_function_final([districts[i],districts[parent_cluster]],weight_pop,weight_race,weight_vote)\n",
    "                    \n",
    "                    #cluster A\n",
    "                    cluster_A=District(districts[i].composition)\n",
    "                    cluster_A.add([vtd])\n",
    "                    \n",
    "                    #cluster B\n",
    "                    cluster_B=District(districts[parent_cluster].composition)\n",
    "                    cluster_B.delete([vtd])\n",
    "                    \n",
    "                    \n",
    "                    new_cost=cost_function_final([cluster_A,cluster_B],weight_pop,weight_race,weight_vote)\n",
    "                    if new_cost<old_cost:\n",
    "                        change+=1\n",
    "                        districts[i].add([vtd])\n",
    "                        districts[parent_cluster].delete([vtd])\n",
    "                    else:\n",
    "                        new_neighbors=VTD(vtd).neighbors\n",
    "                        for vtd_new_neighbors in new_neighbors:\n",
    "                            if vtd_new_neighbors in cluster_A.composition:\n",
    "                                pass\n",
    "                            else:\n",
    "                                parent_cluster_new=get_parent_cluster(districts,vtd_new_neighbors)\n",
    "                                if parent_cluster_new!=parent_cluster:\n",
    "                                    old_cost=cost_function_final([cluster_A,cluster_B,districts[parent_cluster_new]],weight_pop,weight_race,weight_vote)\n",
    "                                    \n",
    "                                    #cluster A\n",
    "                                    cluster_A_prime=District(cluster_A.composition)\n",
    "                                    cluster_A_prime.add([vtd_new_neighbors])\n",
    "                                    \n",
    "                                    #Cluster C\n",
    "                                    cluster_C=District(districts[parent_cluster_new].composition)\n",
    "                                    cluster_C.delete([vtd_new_neighbors])\n",
    "                                    \n",
    "                                    new_cost=cost_function_final([cluster_A_prime,cluster_B,cluster_C],weight_pop,weight_race,weight_vote)\n",
    "                                    \n",
    "                                    if new_cost<old_cost:\n",
    "                                        change+=1\n",
    "                                        districts[i].add([vtd])\n",
    "                                        districts[i].add([vtd_new_neighbors])\n",
    "                                        districts[parent_cluster].delete([vtd])\n",
    "                                        districts[parent_cluster_new].delete([vtd_new_neighbors])\n",
    "                                else:\n",
    "                                    old_cost=cost_function_final([cluster_A,cluster_B],weight_pop,weight_race,weight_vote)\n",
    "                                                                        #cluster A\n",
    "                                    cluster_A_prime=District(cluster_A.composition)\n",
    "                                    cluster_A_prime.add([vtd_new_neighbors])\n",
    "                                    \n",
    "                                    #Cluster C\n",
    "                                    cluster_B_prime=District(cluster_B.composition)\n",
    "                                    cluster_B_prime.delete([vtd_new_neighbors])\n",
    "                                    \n",
    "                                    new_cost=cost_function_final([cluster_A_prime,cluster_B_prime],weight_pop,weight_race,weight_vote)\n",
    "                                    if new_cost<old_cost:\n",
    "                                        change+=1\n",
    "                                        districts[i].add([vtd])\n",
    "                                        districts[i].add([vtd_new_neighbors])\n",
    "                                        districts[parent_cluster].delete([vtd])\n",
    "                                        districts[parent_cluster].delete([vtd_new_neighbors])\n",
    "                                    \n",
    "                    \n",
    "\n",
    "                                \n",
    "                                \n",
    "                        \n",
    "        if change==0:\n",
    "            break\n",
    "        new_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "        return(districts,print_clusters(districts),print('The score has been improved by',curr_cost-new_cost))       \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen previously, one of the main drawbacks of the 2-OPT method is that it is very locally optimal.   \n",
    "While the 2-OPT algorithm only accepts points that lower the objective function, the simulated annealing algorithm accepts not only new points that lower the objective, but also, with a certain probability, points that raise the objective. By doing this, the algorithm avoids being trapped in local minima, and is able to explore globally for more possible solutions. The probability is calculated using a temperature parameter, that will decrease through iterations, reducing the extent of the search and thus assuring that the algorithm will reach a local optimum at some point.\n",
    "\n",
    "__Algorithm__\n",
    "The simulated annealing method needs a feasible solution has an input. We use the random start algorithm to generate it.  \n",
    "__Input:__ Feasible solution V, number of iterations tmax, initial temperature T0, temperature parameter α.\n",
    "0. Let V be our current solution \n",
    "1. For t ranging from 1 to tmax:  \n",
    "(a) Look at every neighbor N of V  \n",
    "(b) If N has a better score than V,V :=N  \n",
    "(c) Otherwise, if we note C(V) and C(N) the costs of solutions V and N, do V := N with a probability\n",
    "e−(C(V )−C(N))/T(t) (1) where T (t) is the temperature at period t defined as T (t) = T0 ∗ αt  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The results provided are as follows in this picture:\n",
    "<img src=\"sim_anne.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_log(t_0,t):\n",
    "    return(t_0/np.log(t))\n",
    " \n",
    "def temp_exp(t_0,t,alpha):\n",
    "    return(t_0*(alpha**t))\n",
    "\n",
    "def sim_annealing(districts,method,alpha=0.1,T_0=1,weight_pop=1,weight_race=1,weight_vote=1,tim=100):\n",
    "    curr_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "    cost=[curr_cost]\n",
    "    for t in range(1,tim):\n",
    "        print('Completion status of main loop: ',t+1,' out of ',tim)\n",
    "        i=random.randint(1,len(districts)-1)\n",
    "        print(i)\n",
    "        to_visit=districts[i].neighbors  \n",
    "        vtd=random.choice(to_visit)\n",
    "        parent_cluster=get_parent_cluster(districts,vtd)\n",
    "        \n",
    "        if is_expendable(districts[parent_cluster],vtd):\n",
    "            old_cost=cost_function_final([districts[i],districts[parent_cluster]],weight_pop,weight_race,weight_vote)\n",
    "            \n",
    "            #cluster A\n",
    "            cluster_A=District(districts[i].composition)\n",
    "            cluster_A.add([vtd])\n",
    "                    \n",
    "            #cluster B\n",
    "            cluster_B=District(districts[parent_cluster].composition)\n",
    "            cluster_B.delete([vtd])\n",
    "            \n",
    "            \n",
    "            new_cost=cost_function_final([cluster_A,cluster_B],weight_pop,weight_race,weight_vote)\n",
    "            \n",
    "            if new_cost<old_cost:\n",
    "                print('normal change')\n",
    "                districts[i].add([vtd])\n",
    "                districts[parent_cluster].delete([vtd])\n",
    "            else:\n",
    "                if method=='log':\n",
    "                    temp=temp_log(T_0,t)\n",
    "                else:\n",
    "                    temp=temp_exp(T_0,t,alpha)\n",
    "                probability=np.exp((old_cost-new_cost)/temp)\n",
    "                if random.random()<probability:\n",
    "                    print('probabilistic change')\n",
    "                    districts[i].add([vtd])\n",
    "                    districts[parent_cluster].delete([vtd])\n",
    "        cost.append(cost_function_final(districts,weight_pop,weight_race,weight_vote))\n",
    "    new_cost=cost_function_final(districts,weight_pop,weight_race,weight_vote)\n",
    "    return(districts,print_clusters(districts),print('The score has been improved by',curr_cost-new_cost),cost)\n",
    "\n",
    "#score to ass the convexity of a solution\n",
    "def convexity_score(shapefile, cluster_results):\n",
    "    for i in range(len(cluster_results)):\n",
    "        shapefile.loc[cluster_results[i].composition,'District']=i\n",
    "    score = 0\n",
    "    clusters = shapefile['District'].unique().tolist()\n",
    "    for cluster_id in clusters:\n",
    "        cluster = shapefile[shapefile['District'] == cluster_id]\n",
    "        union = cluster.unary_union\n",
    "        conv_hull = union.convex_hull\n",
    "        s = union.area / conv_hull.area\n",
    "        score += s    \n",
    "    return score / len(clusters)\n",
    "            \n",
    "            \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is aimed at providing the code and some explanations, for more details about the project please refer to the written report by Alexandru Socolov, Luca Mingardi and I on my github."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
